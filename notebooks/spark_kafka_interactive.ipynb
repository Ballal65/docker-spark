{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "import-libraries",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StructType, StringType, TimestampType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initialize-spark-session",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"NewApp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae5a5934-d739-4560-8525-12b78905a688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://157.245.104.57:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>NewApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7c24b615b820>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "define-schema",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define schema for Kafka messages\n",
    "schema = StructType() \\\n",
    "    .add(\"client_host\", StringType()) \\\n",
    "    .add(\"http_method\", StringType()) \\\n",
    "    .add(\"url\", StringType()) \\\n",
    "    .add(\"event_time\", TimestampType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "169a1215-34c6-4504-a0a6-acaca12cd3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../extracted_data/jre_data_2025-01-22.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "856f95f5-cd1e-4df4-acbb-cffab2d1a87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\", True).option(\"inferSchema\", True).load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48a0f459-587d-4c14-a76c-eeff6f4f4d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "df = df.withColumn('duration_in_minutes', (df['Duration']/60)).drop('Category', 'Tags')\\\n",
    "    .withColumn('jre_episode', when(df['Title'] == 'Joe Rogan Experience%', True).otherwise(False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bd14628-22a2-4067-9c80-b3df005b7d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------+--------------------+-----------+-----------+--------------------+-------------------+-----------+\n",
      "|               Title|        Publish Date|Duration|          Guest Name|View Counts|Like Counts|          Count Date|duration_in_minutes|jre_episode|\n",
      "+--------------------+--------------------+--------+--------------------+-----------+-----------+--------------------+-------------------+-----------+\n",
      "|Joe Rogan Experie...|2025-01-22T18:00:22Z| 12080.0|         Lex Fridman|      24019|       1417|2025-01-22T18:00:22Z| 201.33333333333334|      false|\n",
      "|Joe Rogan Experie...|2025-01-17T18:00:46Z| 10061.0|     Thomas Campbell|    1578684|      31097|2025-01-17T18:00:46Z| 167.68333333333334|      false|\n",
      "|Joe Rogan Experie...|2025-01-16T18:00:14Z| 10074.0|      Steven Rinella|    1162294|      19939|2025-01-16T18:00:14Z|              167.9|      false|\n",
      "|Joe Rogan Experie...|2025-01-15T18:00:44Z|  9981.0|        Bryan Callen|    1217079|      22745|2025-01-15T18:00:44Z|             166.35|      false|\n",
      "|Joe Rogan Experie...|2025-01-10T18:00:12Z| 10236.0|     Mark Zuckerberg|    8160413|     164812|2025-01-10T18:00:12Z|              170.6|      false|\n",
      "|Joe Rogan Experie...|2025-01-09T18:00:02Z|  8459.0|          Mel Gibson|    8731344|     272886|2025-01-09T18:00:02Z| 140.98333333333332|      false|\n",
      "|Joe Rogan Experie...|2025-01-08T18:00:10Z|  7149.0|            Theo Von|    4096526|      76992|2025-01-08T18:00:10Z|             119.15|      false|\n",
      "|Joe Rogan Experie...|2025-01-07T18:00:11Z| 11746.0|         Wesley Huff|    5377233|     159234|2025-01-07T18:00:11Z| 195.76666666666668|      false|\n",
      "|Joe Rogan Experie...|2025-01-02T18:00:37Z|  7629.0|Rick Perry & W. B...|    1425818|      29581|2025-01-02T18:00:37Z|             127.15|      false|\n",
      "|Joe Rogan Experie...|2025-01-01T18:00:15Z|  7338.0|             Raekwon|     828589|      22233|2025-01-01T18:00:15Z|              122.3|      false|\n",
      "|Joe Rogan Experie...|2024-12-31T18:00:09Z| 11306.0|Yannis Pappas & C...|    1849024|      26951|2024-12-31T18:00:09Z| 188.43333333333334|      false|\n",
      "|Joe Rogan Experie...|2024-12-26T18:00:17Z| 10332.0|     Michael Waddell|     990030|      14801|2024-12-26T18:00:17Z|              172.2|      false|\n",
      "|Joe Rogan Experie...|2024-12-25T18:00:14Z|  9303.0|     Duncan Trussell|    2215533|      43838|2024-12-25T18:00:14Z|             155.05|      false|\n",
      "|Joe Rogan Experie...|2024-12-24T18:00:04Z|  9838.0|           James Fox|    1691338|      28111|2024-12-24T18:00:04Z| 163.96666666666667|      false|\n",
      "|Joe Rogan Experie...|2024-12-18T18:00:36Z| 10080.0|     Rod Blagojevich|    1384033|      27230|2024-12-18T18:00:36Z|              168.0|      false|\n",
      "|Joe Rogan Experie...|2024-12-17T18:00:13Z| 10123.0|         Ryan Graves|    2498743|      40927|2024-12-17T18:00:13Z| 168.71666666666667|      false|\n",
      "|Joe Rogan Experie...|2024-12-16T18:00:06Z| 10018.0|       Julian Lennon|     672207|      13314|2024-12-16T18:00:06Z| 166.96666666666667|      false|\n",
      "|Joe Rogan Experie...|2024-12-12T18:00:06Z| 13097.0|          Bert Sorin|     850784|       9762|2024-12-12T18:00:06Z| 218.28333333333333|      false|\n",
      "|Joe Rogan Experie...|2024-12-11T18:00:22Z| 11496.0|      Rick Strassman|     970747|      14002|2024-12-11T18:00:22Z|              191.6|      false|\n",
      "|Joe Rogan Experie...|2024-12-10T18:00:02Z| 11990.0|Roger Avary & Que...|    3882562|      55836|2024-12-10T18:00:02Z| 199.83333333333334|      false|\n",
      "+--------------------+--------------------+--------+--------------------+-----------+-----------+--------------------+-------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "read-from-kafka",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from Kafka\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"ui-event-log\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "parse-transform-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the value column and apply schema\n",
    "parsed_df = df.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "# Example: Transform data (simple transformation)\n",
    "transformed_df = parsed_df.withColumn(\"processed_time\", col(\"event_time\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6f602ffc-7c70-40b9-aac2-2c4f803aa3ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Queries with streaming sources must be executed with writeStream.start();\nkafka",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtransformed_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/sql/dataframe.py:1605\u001b[0m, in \u001b[0;36mDataFrame.head\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1603\u001b[0m     rs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   1604\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m rs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1605\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/sql/dataframe.py:744\u001b[0m, in \u001b[0;36mDataFrame.take\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtake\u001b[39m(\u001b[38;5;28mself\u001b[39m, num):\n\u001b[1;32m    735\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the first ``num`` rows as a :class:`list` of :class:`Row`.\u001b[39;00m\n\u001b[1;32m    736\u001b[0m \n\u001b[1;32m    737\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[38;5;124;03m    [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001b[39;00m\n\u001b[1;32m    743\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/sql/dataframe.py:693\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m    684\u001b[0m \n\u001b[1;32m    685\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;124;03m[Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001b[39;00m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc) \u001b[38;5;28;01mas\u001b[39;00m css:\n\u001b[0;32m--> 693\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(PickleSerializer())))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Queries with streaming sources must be executed with writeStream.start();\nkafka"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "display-transformed-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "25/03/15 07:16:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/15 07:16:28 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o38.showString.\n: java.lang.NoClassDefFoundError: org/apache/spark/kafka010/KafkaConfigUpdater\n\tat org.apache.spark.sql.kafka010.KafkaSourceProvider$.kafkaParamsForDriver(KafkaSourceProvider.scala:643)\n\tat org.apache.spark.sql.kafka010.KafkaRelation.buildScan(KafkaRelation.scala:61)\n\tat org.apache.spark.sql.execution.datasources.DataSourceStrategy$.apply(DataSourceStrategy.scala:339)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$1(QueryPlanner.scala:63)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:68)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:68)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:68)\n\tat org.apache.spark.sql.execution.QueryExecution$.createSparkPlan(QueryExecution.scala:468)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$sparkPlan$2(QueryExecution.scala:157)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:196)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:196)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$sparkPlan$1(QueryExecution.scala:157)\n\tat org.apache.spark.sql.execution.QueryExecution.withCteMap(QueryExecution.scala:73)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:150)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:150)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$2(QueryExecution.scala:170)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:196)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:196)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$1(QueryExecution.scala:170)\n\tat org.apache.spark.sql.execution.QueryExecution.withCteMap(QueryExecution.scala:73)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:163)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:163)\n\tat org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:214)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:259)\n\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:228)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2728)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2935)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:287)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:326)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.ClassNotFoundException: org.apache.spark.kafka010.KafkaConfigUpdater\n\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581)\n\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)\n\t... 81 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 17\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Simple Kafka read (batch, not streaming)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m df \u001b[38;5;241m=\u001b[39m spark \\\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;241m.\u001b[39mread \\\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka.bootstrap.servers\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka:9092\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubscribe\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAIDHOOSTATION\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m---> 17\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m spark\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/sql/dataframe.py:494\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a bool\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 494\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o38.showString.\n: java.lang.NoClassDefFoundError: org/apache/spark/kafka010/KafkaConfigUpdater\n\tat org.apache.spark.sql.kafka010.KafkaSourceProvider$.kafkaParamsForDriver(KafkaSourceProvider.scala:643)\n\tat org.apache.spark.sql.kafka010.KafkaRelation.buildScan(KafkaRelation.scala:61)\n\tat org.apache.spark.sql.execution.datasources.DataSourceStrategy$.apply(DataSourceStrategy.scala:339)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$1(QueryPlanner.scala:63)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:68)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:68)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:68)\n\tat org.apache.spark.sql.execution.QueryExecution$.createSparkPlan(QueryExecution.scala:468)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$sparkPlan$2(QueryExecution.scala:157)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:196)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:196)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$sparkPlan$1(QueryExecution.scala:157)\n\tat org.apache.spark.sql.execution.QueryExecution.withCteMap(QueryExecution.scala:73)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:150)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:150)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$2(QueryExecution.scala:170)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:196)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:196)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$1(QueryExecution.scala:170)\n\tat org.apache.spark.sql.execution.QueryExecution.withCteMap(QueryExecution.scala:73)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:163)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:163)\n\tat org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:214)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:259)\n\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:228)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2728)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2935)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:287)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:326)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.ClassNotFoundException: org.apache.spark.kafka010.KafkaConfigUpdater\n\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581)\n\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)\n\t... 81 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Minimal Spark session with explicit JAR\n",
    "# 'spark-sql-kafka-0-10_2.12-3.2.1.jar''\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaTest\") \\\n",
    "    .config(\"spark.jars\", \"/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.2.1.jar\") \\\n",
    "    .getOrCreate() \n",
    "\n",
    "# Simple Kafka read (batch, not streaming)\n",
    "df = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"AIDHOOSTATION\") \\\n",
    "    .load()\n",
    "\n",
    "df.show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67c42a40-ea8e-4377-8ac2-66a0b6ba8165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hive-shims-scheduler-2.3.9.jar', 'hk2-api-2.6.1.jar', 'jakarta.validation-api-2.0.2.jar', 'commons-logging-1.1.3.jar', 'audience-annotations-0.5.0.jar', 'logging-interceptor-3.12.12.jar', 'antlr4-runtime-4.8.jar', 'spark-hive_2.12-3.2.1.jar', 'jaxb-api-2.2.11.jar', 'scala-library-2.12.15.jar', 'jdom-1.1.jar', 'hadoop-cloud-storage-3.3.1.jar', 'spark-mesos_2.12-3.2.1.jar', 'azure-data-lake-store-sdk-2.3.9.jar', 'breeze-macros_2.12-1.2.jar', 'breeze_2.12-1.2.jar', 'aopalliance-repackaged-2.6.1.jar', 'commons-collections-3.2.2.jar', 'kubernetes-model-discovery-5.4.1.jar', 'spire-macros_2.12-0.17.0.jar', 'bonecp-0.8.0.RELEASE.jar', 'dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar', 'spark-unsafe_2.12-3.2.1.jar', 'commons-text-1.6.jar', 'spark-graphx_2.12-3.2.1.jar', 'arpack-2.2.1.jar', 'commons-crypto-1.1.0.jar', 'zjsonpatch-0.3.0.jar', 'pyrolite-4.30.jar', 'metrics-json-4.2.0.jar', 'zstd-jni-1.5.0-4.jar', 'jersey-container-servlet-2.34.jar', 'json4s-core_2.12-3.7.0-M11.jar', 'hk2-utils-2.6.1.jar', 'htrace-core4-4.1.0-incubating.jar', 'minlog-1.3.0.jar', 'JTransforms-3.1.jar', 'jaxb-runtime-2.3.2.jar', 'parquet-encoding-1.12.2.jar', 'json4s-jackson_2.12-3.7.0-M11.jar', 'curator-client-2.13.0.jar', 'hive-service-rpc-3.1.2.jar', 'wildfly-openssl-1.0.7.Final.jar', 'joda-time-2.10.10.jar', 'jetty-util-ajax-9.4.43.v20210629.jar', 'threeten-extra-1.5.0.jar', 'jpam-1.1.jar', 'kubernetes-model-metrics-5.4.1.jar', 'slf4j-api-1.7.30.jar', 'curator-recipes-2.13.0.jar', 'shapeless_2.12-2.3.3.jar', 'jta-1.1.jar', 'hive-vector-code-gen-2.3.9.jar', 'ion-java-1.0.2.jar', 'kubernetes-model-apps-5.4.1.jar', 'jersey-container-servlet-core-2.34.jar', 'super-csv-2.2.0.jar', 'jcl-over-slf4j-1.7.30.jar', 'univocity-parsers-2.9.1.jar', 'jackson-datatype-jsr310-2.11.2.jar', 'lapack-2.2.1.jar', 'ivy-2.5.0.jar', 'commons-dbcp-1.4.jar', 'istack-commons-runtime-3.0.8.jar', 'jline-2.14.6.jar', 'kubernetes-model-admissionregistration-5.4.1.jar', 'hive-storage-api-2.7.2.jar', 'shims-common-2.3.9.jar', 'spire-platform_2.12-0.17.0.jar', 'JLargeArrays-1.5.jar', 'slf4j-log4j12-1.7.30.jar', 'kubernetes-model-scheduling-5.4.1.jar', 'kubernetes-model-apiextensions-5.4.1.jar', 'kubernetes-model-storageclass-5.4.1.jar', 'javolution-5.5.1.jar', 'macro-compat_2.12-1.1.1.jar', 'hive2-shims-2.3.9.jar', 'aws-glue-datacatalog-spark-client-2.3.9.jar', 'delta-core_2.12-1.1.0.jar', 'jackson-mapper-asl-1.9.13.jar', 'scala-reflect-2.12.15.jar', 'metrics-core-4.2.0.jar', 'kubernetes-client-5.4.1.jar', 'jackson-core-2.12.3.jar', 'transaction-api-1.1.jar', 'spark-mllib_2.12-3.2.1.jar', 'netty-all-4.1.68.Final.jar', 'hive-jdbc-2.3.9.jar', 'jul-to-slf4j-1.7.30.jar', 'okhttp-3.12.12.jar', 'gson-2.2.4.jar', 'spire_2.12-0.17.0.jar', 'avro-1.10.2.jar', 'kubernetes-model-networking-5.4.1.jar', 'hadoop-aws-3.3.1.jar', 'libfb303-0.9.3.jar', 'compress-lzf-1.0.3.jar', 'ST4-4.0.4.jar', 'jodd-core-3.5.2.jar', 'lz4-java-1.7.1.jar', 'py4j-0.10.9.3.jar', 'jetty-util-9.4.43.v20210629.jar', 'spark-streaming_2.12-3.2.1.jar', 'jsr305-3.0.0.jar', 'hadoop-shaded-guava-1.1.1.jar', 'hadoop-aliyun-3.3.1.jar', 'metrics-jmx-4.2.0.jar', 'arpack_combined_all-0.1.jar', 'aws-java-sdk-core-1.11.797.jar', 'hive-shims-2.3.9.jar', 'activation-1.1.1.jar', 'annotations-17.0.0.jar', 'okio-1.14.0.jar', 'hadoop-azure-3.3.1.jar', 'kubernetes-model-flowcontrol-5.4.1.jar', 'aliyun-java-sdk-ecs-4.2.0.jar', 'azure-keyvault-core-1.0.0.jar', 'hive-cli-2.3.9.jar', 'jackson-annotations-2.12.3.jar', 'spark-repl_2.12-3.2.1.jar', 'jakarta.inject-2.6.1.jar', 'aliyun-java-sdk-core-3.4.0.jar', 'jackson-databind-2.12.3.jar', 'kubernetes-model-policy-5.4.1.jar', 'arrow-memory-netty-2.0.0.jar', 'jakarta.servlet-api-4.0.3.jar', 'hive-metastore-2.3.9.jar', 'jakarta.ws.rs-api-2.1.6.jar', 'jdo-api-3.0.1.jar', 'aws-java-sdk-bundle-1.11.901.jar', 'javax.jdo-3.2.0-m3.jar', 'datanucleus-rdbms-4.1.19.jar', 'commons-compress-1.21.jar', 'kubernetes-model-core-5.4.1.jar', 'spire-util_2.12-0.17.0.jar', 'avro-ipc-1.10.2.jar', 'kubernetes-model-autoscaling-5.4.1.jar', 'curator-framework-2.13.0.jar', 'scala-xml_2.12-1.2.0.jar', 'spark-yarn_2.12-3.2.1.jar', 'jackson-module-scala_2.12-2.12.3.jar', 'core-1.1.2.jar', 'commons-lang3-3.12.0.jar', 'commons-codec-1.15.jar', 'kryo-shaded-4.0.2.jar', 'commons-cli-1.2.jar', 'arrow-memory-core-2.0.0.jar', 'jakarta.annotation-api-1.3.5.jar', 'shims-0.9.0.jar', 'hive-shims-0.23-2.3.9.jar', 'shims-loader-2.3.9.jar', 'hadoop-client-api-3.3.1.jar', 'kubernetes-model-batch-5.4.1.jar', 'stream-2.9.6.jar', 'metrics-jvm-4.2.0.jar', 'hive-exec-2.3.9-core.jar', 'xbean-asm9-shaded-4.20.jar', 'janino-3.0.16.jar', 'kubernetes-model-rbac-5.4.1.jar', 'protobuf-java-2.5.0.jar', 'spark-hive-thriftserver_2.12-3.2.1.jar', 'paranamer-2.8.jar', 'kubernetes-model-events-5.4.1.jar', 'spark-tags_2.12-3.2.1.jar', 'httpclient-4.5.13.jar', 'jackson-dataformat-yaml-2.12.3.jar', 'parquet-column-1.12.2.jar', 'json-1.8.jar', 'kubernetes-model-certificates-5.4.1.jar', 'spark-tags_2.12-3.2.1-tests.jar', 'spark-mllib-local_2.12-3.2.1.jar', 'hive-beeline-2.3.9.jar', 'spark-kvstore_2.12-3.2.1.jar', 'aliyun-java-sdk-sts-3.0.0.jar', 'parquet-hadoop-1.12.2.jar', 'zookeeper-3.6.2.jar', 'kubernetes-model-extensions-5.4.1.jar', 'commons-compiler-3.0.16.jar', 'spark-network-shuffle_2.12-3.2.1.jar', 'hadoop-client-runtime-3.3.1.jar', 'chill_2.12-0.10.0.jar', 'tink-1.6.0.jar', 'hadoop-annotations-3.3.1.jar', 'scala-compiler-2.12.15.jar', 'hive-shims-common-2.3.9.jar', 'derby-10.14.2.0.jar', 'velocity-1.5.jar', 'automaton-1.11-8.jar', 'kubernetes-model-common-5.4.1.jar', 'httpcore-4.4.14.jar', 'stax-api-1.0.1.jar', 'rocksdbjni-6.20.3.jar', 'oro-2.0.8.jar', 'mesos-1.4.0-shaded-protobuf.jar', 'avro-mapred-1.10.2.jar', 'hadoop-cos-3.3.1.jar', 'jersey-client-2.34.jar', 'antlr-runtime-3.5.2.jar', 'commons-pool-1.5.4.jar', 'arrow-format-2.0.0.jar', 'zookeeper-jute-3.6.2.jar', 'javassist-3.25.0-GA.jar', 'hive-common-2.3.9.jar', 'jakarta.xml.bind-api-2.3.2.jar', 'spark-sketch_2.12-3.2.1.jar', 'kubernetes-model-coordination-5.4.1.jar', 'snappy-java-1.1.8.4.jar', 'opencsv-2.3.jar', 'jettison-1.1.jar', 'jersey-hk2-2.34.jar', 'hadoop-azure-datalake-3.3.1.jar', 'flatbuffers-java-1.9.0.jar', 'aliyun-sdk-oss-3.4.1.jar', 'orc-shims-1.6.12.jar', 'parquet-jackson-1.12.2.jar', 'aliyun-java-sdk-ram-3.0.0.jar', 'commons-lang-2.6.jar', 'spark-hadoop-cloud_2.12-3.2.1.jar', 'hive-serde-2.3.9.jar', 'spark-hive-shims-2.3.9.jar', 'spark-network-common_2.12-3.2.1.jar', 'osgi-resource-locator-1.0.3.jar', 'jackson-core-asl-1.9.13.jar', 'objenesis-2.6.jar', 'spark-snowflake_2.12-2.10.0-spark_3.2.jar', 'xz-1.8.jar', 'snowflake-ingest-sdk-0.10.3.jar', 'hadoop-openstack-3.3.1.jar', 'parquet-format-structures-1.12.2.jar', 'HikariCP-2.5.1.jar', 'scala-parser-combinators_2.12-1.1.2.jar', 'jersey-server-2.34.jar', 'blas-2.2.1.jar', 'cats-kernel_2.12-2.1.1.jar', 'hadoop-yarn-server-web-proxy-3.3.1.jar', 'json4s-scalap_2.12-3.7.0-M11.jar', 'arrow-vector-2.0.0.jar', 'json4s-ast_2.12-3.7.0-M11.jar', 'datanucleus-api-jdo-4.2.4.jar', 'azure-storage-7.0.1.jar', 'jmespath-java-1.11.797.jar', 'RoaringBitmap-0.9.0.jar', 'jersey-common-2.34.jar', 'aws-glue-datacatalog-client-common-2.3.9.jar', 'leveldbjni-all-1.8.jar', 'spark-kubernetes_2.12-3.2.1.jar', 'aws-java-sdk-glue-1.11.797.jar', 'spark-core_2.12-3.2.1.jar', 'metrics-graphite-4.2.0.jar', 'libthrift-0.12.0.jar', 'parquet-common-1.12.2.jar', 'scala-collection-compat_2.12-2.1.1.jar', 'orc-mapreduce-1.6.12.jar', 'cos_api-bundle-5.6.19.jar', 'spark-sql_2.12-3.2.1.jar', 'spark-launcher_2.12-3.2.1.jar', 'chill-java-0.10.0.jar', 'commons-io-2.8.0.jar', 'algebra_2.12-2.0.1.jar', 'log4j-1.2.17.jar', 'jetty-util-9.4.43.v20210629.jar.1', 'orc-core-1.6.12.jar', 'snakeyaml-1.27.jar', 'spark-catalyst_2.12-3.2.1.jar', 'hive-llap-common-2.3.9.jar', 'commons-net-3.1.jar', 'datanucleus-core-4.1.17.jar', 'generex-1.0.2.jar', 'snowflake-jdbc-3.13.14.jar', 'hk2-locator-2.6.1.jar', 'kubernetes-model-node-5.4.1.jar', 'aircompressor-0.21.jar', 'jackson-dataformat-cbor-2.12.3.jar', 'commons-math3-3.4.1.jar', 'kafka-clients-3.2.1.jar', 'spark-sql-kafka-0-10_2.12-3.2.1.jar', 'postgresql-42.3.1.jar', 'gcs-connector-hadoop3-2.1.5.jar', 'guava-29.0-jre.jar']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir('/opt/spark/jars/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8c89e64-c480-4ac5-84e2-f76e5ba96769",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"AIDHOOSTATION\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2395217a-a1e2-45dc-951a-348815b8fb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/15 07:37:50 DEBUG GenerateUnsafeProjection: code for input[0, binary, true],input[1, binary, true],input[2, string, true],input[3, int, true],input[4, bigint, true],input[5, timestamp, true],input[6, int, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(7, 96);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */     writeFields_0_0(i);\n",
      "/* 031 */     writeFields_0_1(i);\n",
      "/* 032 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 033 */   }\n",
      "/* 034 */\n",
      "/* 035 */\n",
      "/* 036 */   private void writeFields_0_1(InternalRow i) {\n",
      "/* 037 */\n",
      "/* 038 */     boolean isNull_5 = i.isNullAt(5);\n",
      "/* 039 */     long value_5 = isNull_5 ?\n",
      "/* 040 */     -1L : (i.getLong(5));\n",
      "/* 041 */     if (isNull_5) {\n",
      "/* 042 */       mutableStateArray_0[0].setNullAt(5);\n",
      "/* 043 */     } else {\n",
      "/* 044 */       mutableStateArray_0[0].write(5, value_5);\n",
      "/* 045 */     }\n",
      "/* 046 */\n",
      "/* 047 */     boolean isNull_6 = i.isNullAt(6);\n",
      "/* 048 */     int value_6 = isNull_6 ?\n",
      "/* 049 */     -1 : (i.getInt(6));\n",
      "/* 050 */     if (isNull_6) {\n",
      "/* 051 */       mutableStateArray_0[0].setNullAt(6);\n",
      "/* 052 */     } else {\n",
      "/* 053 */       mutableStateArray_0[0].write(6, value_6);\n",
      "/* 054 */     }\n",
      "/* 055 */\n",
      "/* 056 */   }\n",
      "/* 057 */\n",
      "/* 058 */\n",
      "/* 059 */   private void writeFields_0_0(InternalRow i) {\n",
      "/* 060 */\n",
      "/* 061 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 062 */     byte[] value_0 = isNull_0 ?\n",
      "/* 063 */     null : (i.getBinary(0));\n",
      "/* 064 */     if (isNull_0) {\n",
      "/* 065 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 066 */     } else {\n",
      "/* 067 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 068 */     }\n",
      "/* 069 */\n",
      "/* 070 */     boolean isNull_1 = i.isNullAt(1);\n",
      "/* 071 */     byte[] value_1 = isNull_1 ?\n",
      "/* 072 */     null : (i.getBinary(1));\n",
      "/* 073 */     if (isNull_1) {\n",
      "/* 074 */       mutableStateArray_0[0].setNullAt(1);\n",
      "/* 075 */     } else {\n",
      "/* 076 */       mutableStateArray_0[0].write(1, value_1);\n",
      "/* 077 */     }\n",
      "/* 078 */\n",
      "/* 079 */     boolean isNull_2 = i.isNullAt(2);\n",
      "/* 080 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 081 */     null : (i.getUTF8String(2));\n",
      "/* 082 */     if (isNull_2) {\n",
      "/* 083 */       mutableStateArray_0[0].setNullAt(2);\n",
      "/* 084 */     } else {\n",
      "/* 085 */       mutableStateArray_0[0].write(2, value_2);\n",
      "/* 086 */     }\n",
      "/* 087 */\n",
      "/* 088 */     boolean isNull_3 = i.isNullAt(3);\n",
      "/* 089 */     int value_3 = isNull_3 ?\n",
      "/* 090 */     -1 : (i.getInt(3));\n",
      "/* 091 */     if (isNull_3) {\n",
      "/* 092 */       mutableStateArray_0[0].setNullAt(3);\n",
      "/* 093 */     } else {\n",
      "/* 094 */       mutableStateArray_0[0].write(3, value_3);\n",
      "/* 095 */     }\n",
      "/* 096 */\n",
      "/* 097 */     boolean isNull_4 = i.isNullAt(4);\n",
      "/* 098 */     long value_4 = isNull_4 ?\n",
      "/* 099 */     -1L : (i.getLong(4));\n",
      "/* 100 */     if (isNull_4) {\n",
      "/* 101 */       mutableStateArray_0[0].setNullAt(4);\n",
      "/* 102 */     } else {\n",
      "/* 103 */       mutableStateArray_0[0].write(4, value_4);\n",
      "/* 104 */     }\n",
      "/* 105 */\n",
      "/* 106 */   }\n",
      "/* 107 */\n",
      "/* 108 */ }\n",
      "\n",
      "25/03/15 07:37:50 DEBUG GenerateUnsafeProjection: code for input[0, binary, true],input[1, binary, true],input[2, string, true],input[3, int, true],input[4, bigint, true],input[5, timestamp, true],input[6, int, true],input[7, array<struct<key:string,value:binary>>, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];\n",
      "/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter[] mutableStateArray_1 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter[1];\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(8, 128);\n",
      "/* 014 */     mutableStateArray_1[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter(mutableStateArray_0[0], 8);\n",
      "/* 015 */     mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(mutableStateArray_1[0], 2);\n",
      "/* 016 */\n",
      "/* 017 */   }\n",
      "/* 018 */\n",
      "/* 019 */   public void initialize(int partitionIndex) {\n",
      "/* 020 */\n",
      "/* 021 */   }\n",
      "/* 022 */\n",
      "/* 023 */   // Scala.Function1 need this\n",
      "/* 024 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 025 */     return apply((InternalRow) row);\n",
      "/* 026 */   }\n",
      "/* 027 */\n",
      "/* 028 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 029 */     mutableStateArray_0[0].reset();\n",
      "/* 030 */\n",
      "/* 031 */\n",
      "/* 032 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 033 */     writeFields_0_0(i);\n",
      "/* 034 */     writeFields_0_1(i);\n",
      "/* 035 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 036 */   }\n",
      "/* 037 */\n",
      "/* 038 */\n",
      "/* 039 */   private void writeFields_0_1(InternalRow i) {\n",
      "/* 040 */\n",
      "/* 041 */     boolean isNull_5 = i.isNullAt(5);\n",
      "/* 042 */     long value_5 = isNull_5 ?\n",
      "/* 043 */     -1L : (i.getLong(5));\n",
      "/* 044 */     if (isNull_5) {\n",
      "/* 045 */       mutableStateArray_0[0].setNullAt(5);\n",
      "/* 046 */     } else {\n",
      "/* 047 */       mutableStateArray_0[0].write(5, value_5);\n",
      "/* 048 */     }\n",
      "/* 049 */\n",
      "/* 050 */     boolean isNull_6 = i.isNullAt(6);\n",
      "/* 051 */     int value_6 = isNull_6 ?\n",
      "/* 052 */     -1 : (i.getInt(6));\n",
      "/* 053 */     if (isNull_6) {\n",
      "/* 054 */       mutableStateArray_0[0].setNullAt(6);\n",
      "/* 055 */     } else {\n",
      "/* 056 */       mutableStateArray_0[0].write(6, value_6);\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     boolean isNull_7 = i.isNullAt(7);\n",
      "/* 060 */     ArrayData value_7 = isNull_7 ?\n",
      "/* 061 */     null : (i.getArray(7));\n",
      "/* 062 */     if (isNull_7) {\n",
      "/* 063 */       mutableStateArray_0[0].setNullAt(7);\n",
      "/* 064 */     } else {\n",
      "/* 065 */       // Remember the current cursor so that we can calculate how many bytes are\n",
      "/* 066 */       // written later.\n",
      "/* 067 */       final int previousCursor_0 = mutableStateArray_0[0].cursor();\n",
      "/* 068 */\n",
      "/* 069 */       final ArrayData tmpInput_0 = value_7;\n",
      "/* 070 */       if (tmpInput_0 instanceof UnsafeArrayData) {\n",
      "/* 071 */         mutableStateArray_0[0].write((UnsafeArrayData) tmpInput_0);\n",
      "/* 072 */       } else {\n",
      "/* 073 */         final int numElements_0 = tmpInput_0.numElements();\n",
      "/* 074 */         mutableStateArray_1[0].initialize(numElements_0);\n",
      "/* 075 */\n",
      "/* 076 */         for (int index_0 = 0; index_0 < numElements_0; index_0++) {\n",
      "/* 077 */\n",
      "/* 078 */           if (tmpInput_0.isNullAt(index_0)) {\n",
      "/* 079 */             mutableStateArray_1[0].setNull8Bytes(index_0);\n",
      "/* 080 */           } else {\n",
      "/* 081 */\n",
      "/* 082 */             final InternalRow tmpInput_1 = tmpInput_0.getStruct(index_0, 2);\n",
      "/* 083 */             if (tmpInput_1 instanceof UnsafeRow) {\n",
      "/* 084 */               mutableStateArray_1[0].write(index_0, (UnsafeRow) tmpInput_1);\n",
      "/* 085 */             } else {\n",
      "/* 086 */               // Remember the current cursor so that we can calculate how many bytes are\n",
      "/* 087 */               // written later.\n",
      "/* 088 */               final int previousCursor_1 = mutableStateArray_1[0].cursor();\n",
      "/* 089 */\n",
      "/* 090 */               mutableStateArray_0[1].resetRowWriter();\n",
      "/* 091 */\n",
      "/* 092 */\n",
      "/* 093 */               if ((tmpInput_1.isNullAt(0))) {\n",
      "/* 094 */                 mutableStateArray_0[1].setNullAt(0);\n",
      "/* 095 */               } else {\n",
      "/* 096 */                 mutableStateArray_0[1].write(0, (tmpInput_1.getUTF8String(0)));\n",
      "/* 097 */               }\n",
      "/* 098 */\n",
      "/* 099 */\n",
      "/* 100 */               if ((tmpInput_1.isNullAt(1))) {\n",
      "/* 101 */                 mutableStateArray_0[1].setNullAt(1);\n",
      "/* 102 */               } else {\n",
      "/* 103 */                 mutableStateArray_0[1].write(1, (tmpInput_1.getBinary(1)));\n",
      "/* 104 */               }\n",
      "/* 105 */\n",
      "/* 106 */\n",
      "/* 107 */               mutableStateArray_1[0].setOffsetAndSizeFromPreviousCursor(index_0, previousCursor_1);\n",
      "/* 108 */             }\n",
      "/* 109 */\n",
      "/* 110 */           }\n",
      "/* 111 */\n",
      "/* 112 */         }\n",
      "/* 113 */       }\n",
      "/* 114 */\n",
      "/* 115 */       mutableStateArray_0[0].setOffsetAndSizeFromPreviousCursor(7, previousCursor_0);\n",
      "/* 116 */     }\n",
      "/* 117 */\n",
      "/* 118 */   }\n",
      "/* 119 */\n",
      "/* 120 */\n",
      "/* 121 */   private void writeFields_0_0(InternalRow i) {\n",
      "/* 122 */\n",
      "/* 123 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 124 */     byte[] value_0 = isNull_0 ?\n",
      "/* 125 */     null : (i.getBinary(0));\n",
      "/* 126 */     if (isNull_0) {\n",
      "/* 127 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 128 */     } else {\n",
      "/* 129 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 130 */     }\n",
      "/* 131 */\n",
      "/* 132 */     boolean isNull_1 = i.isNullAt(1);\n",
      "/* 133 */     byte[] value_1 = isNull_1 ?\n",
      "/* 134 */     null : (i.getBinary(1));\n",
      "/* 135 */     if (isNull_1) {\n",
      "/* 136 */       mutableStateArray_0[0].setNullAt(1);\n",
      "/* 137 */     } else {\n",
      "/* 138 */       mutableStateArray_0[0].write(1, value_1);\n",
      "/* 139 */     }\n",
      "/* 140 */\n",
      "/* 141 */     boolean isNull_2 = i.isNullAt(2);\n",
      "/* 142 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 143 */     null : (i.getUTF8String(2));\n",
      "/* 144 */     if (isNull_2) {\n",
      "/* 145 */       mutableStateArray_0[0].setNullAt(2);\n",
      "/* 146 */     } else {\n",
      "/* 147 */       mutableStateArray_0[0].write(2, value_2);\n",
      "/* 148 */     }\n",
      "/* 149 */\n",
      "/* 150 */     boolean isNull_3 = i.isNullAt(3);\n",
      "/* 151 */     int value_3 = isNull_3 ?\n",
      "/* 152 */     -1 : (i.getInt(3));\n",
      "/* 153 */     if (isNull_3) {\n",
      "/* 154 */       mutableStateArray_0[0].setNullAt(3);\n",
      "/* 155 */     } else {\n",
      "/* 156 */       mutableStateArray_0[0].write(3, value_3);\n",
      "/* 157 */     }\n",
      "/* 158 */\n",
      "/* 159 */     boolean isNull_4 = i.isNullAt(4);\n",
      "/* 160 */     long value_4 = isNull_4 ?\n",
      "/* 161 */     -1L : (i.getLong(4));\n",
      "/* 162 */     if (isNull_4) {\n",
      "/* 163 */       mutableStateArray_0[0].setNullAt(4);\n",
      "/* 164 */     } else {\n",
      "/* 165 */       mutableStateArray_0[0].write(4, value_4);\n",
      "/* 166 */     }\n",
      "/* 167 */\n",
      "/* 168 */   }\n",
      "/* 169 */\n",
      "/* 170 */ }\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o85.javaToPython.\n: java.lang.NoClassDefFoundError: org/apache/spark/kafka010/KafkaConfigUpdater\n\tat org.apache.spark.sql.kafka010.KafkaSourceProvider$.kafkaParamsForDriver(KafkaSourceProvider.scala:643)\n\tat org.apache.spark.sql.kafka010.KafkaRelation.buildScan(KafkaRelation.scala:61)\n\tat org.apache.spark.sql.execution.datasources.DataSourceStrategy$.apply(DataSourceStrategy.scala:339)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$1(QueryPlanner.scala:63)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:68)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:68)\n\tat org.apache.spark.sql.execution.QueryExecution$.createSparkPlan(QueryExecution.scala:468)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$sparkPlan$2(QueryExecution.scala:157)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:196)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:196)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$sparkPlan$1(QueryExecution.scala:157)\n\tat org.apache.spark.sql.execution.QueryExecution.withCteMap(QueryExecution.scala:73)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:150)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:150)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$2(QueryExecution.scala:170)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:196)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:196)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$1(QueryExecution.scala:170)\n\tat org.apache.spark.sql.execution.QueryExecution.withCteMap(QueryExecution.scala:73)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:163)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:163)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:185)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:184)\n\tat org.apache.spark.sql.Dataset.javaToPython(Dataset.scala:3529)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 17\u001b[0m\n\u001b[1;32m      9\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka.bootstrap.servers\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka:9092\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubscribe\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAIDHOOSTATION\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstartingOffsets\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mearliest\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfailOnDataLoss\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka.security.protocol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPLAINTEXT\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Check if DF is empty first\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[38;5;241m.\u001b[39misEmpty():\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo data found in Kafka topic!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/sql/dataframe.py:86\u001b[0m, in \u001b[0;36mDataFrame.rdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the content as an :class:`pyspark.RDD` of :class:`Row`.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_rdd \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m     jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjavaToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_rdd \u001b[38;5;241m=\u001b[39m RDD(jrdd, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msql_ctx\u001b[38;5;241m.\u001b[39m_sc, BatchedSerializer(PickleSerializer()))\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_rdd\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o85.javaToPython.\n: java.lang.NoClassDefFoundError: org/apache/spark/kafka010/KafkaConfigUpdater\n\tat org.apache.spark.sql.kafka010.KafkaSourceProvider$.kafkaParamsForDriver(KafkaSourceProvider.scala:643)\n\tat org.apache.spark.sql.kafka010.KafkaRelation.buildScan(KafkaRelation.scala:61)\n\tat org.apache.spark.sql.execution.datasources.DataSourceStrategy$.apply(DataSourceStrategy.scala:339)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$1(QueryPlanner.scala:63)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:68)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:68)\n\tat org.apache.spark.sql.execution.QueryExecution$.createSparkPlan(QueryExecution.scala:468)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$sparkPlan$2(QueryExecution.scala:157)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:196)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:196)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$sparkPlan$1(QueryExecution.scala:157)\n\tat org.apache.spark.sql.execution.QueryExecution.withCteMap(QueryExecution.scala:73)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:150)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:150)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$2(QueryExecution.scala:170)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:196)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:196)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$1(QueryExecution.scala:170)\n\tat org.apache.spark.sql.execution.QueryExecution.withCteMap(QueryExecution.scala:73)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:163)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:163)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:185)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:184)\n\tat org.apache.spark.sql.Dataset.javaToPython(Dataset.scala:3529)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaDebug\") \\\n",
    "    .config(\"spark.jars\", \"/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.2.1.jar\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-Dlog4j.configuration=file:/path/to/log4j-debug.properties\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"AIDHOOSTATION\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .option(\"kafka.security.protocol\", \"PLAINTEXT\").load()\n",
    "\n",
    "# Check if DF is empty first\n",
    "if df.rdd.isEmpty():\n",
    "    print(\"No data found in Kafka topic!\")\n",
    "else:\n",
    "    df.show(truncate=False)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a637a8c-61da-4adf-91ac-439b720e9692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/15 07:41:52 INFO SparkUI: Stopped Spark web UI at http://159.89.165.171:4041\n",
      "25/03/15 07:41:52 INFO StandaloneSchedulerBackend: Shutting down all executors\n",
      "25/03/15 07:41:52 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down\n",
      "25/03/15 07:41:52 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "25/03/15 07:41:52 INFO MemoryStore: MemoryStore cleared\n",
      "25/03/15 07:41:52 INFO BlockManager: BlockManager stopped\n",
      "25/03/15 07:41:52 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "25/03/15 07:41:52 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "25/03/15 07:41:52 INFO SparkContext: Successfully stopped SparkContext\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ffc620a-e382-4ab8-b949-bc3fd0b05ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/15 07:42:22 INFO SparkContext: Running Spark version 3.2.1\n",
      "25/03/15 07:42:22 INFO ResourceUtils: ==============================================================\n",
      "25/03/15 07:42:22 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "25/03/15 07:42:22 INFO ResourceUtils: ==============================================================\n",
      "25/03/15 07:42:22 INFO SparkContext: Submitted application: KafkaTest\n",
      "25/03/15 07:42:22 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 2, script: , vendor: , memory -> name: memory, amount: 4096, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "25/03/15 07:42:22 INFO ResourceProfile: Limiting resource is cpus at 2 tasks per executor\n",
      "25/03/15 07:42:22 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "25/03/15 07:42:22 INFO SecurityManager: Changing view acls to: root\n",
      "25/03/15 07:42:22 INFO SecurityManager: Changing modify acls to: root\n",
      "25/03/15 07:42:22 INFO SecurityManager: Changing view acls groups to: \n",
      "25/03/15 07:42:22 INFO SecurityManager: Changing modify acls groups to: \n",
      "25/03/15 07:42:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
      "25/03/15 07:42:22 DEBUG TransportServer: Shuffle server started on port: 35039\n",
      "25/03/15 07:42:22 INFO Utils: Successfully started service 'sparkDriver' on port 35039.\n",
      "25/03/15 07:42:22 DEBUG SparkEnv: Using serializer: class org.apache.spark.serializer.JavaSerializer\n",
      "25/03/15 07:42:22 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/03/15 07:42:22 DEBUG MapOutputTrackerMasterEndpoint: init\n",
      "25/03/15 07:42:22 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/03/15 07:42:22 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "25/03/15 07:42:22 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "25/03/15 07:42:22 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/03/15 07:42:22 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b802111a-3248-40c5-be34-23465f8bbf0f\n",
      "25/03/15 07:42:22 DEBUG DiskBlockManager: Adding shutdown hook\n",
      "25/03/15 07:42:22 INFO MemoryStore: MemoryStore started with capacity 1048.8 MiB\n",
      "25/03/15 07:42:22 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "25/03/15 07:42:22 DEBUG OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: init\n",
      "25/03/15 07:42:22 DEBUG SecurityManager: Created SSL options for ui: SSLOptions{enabled=false, port=None, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}\n",
      "25/03/15 07:42:22 DEBUG JettyUtils: Using requestHeaderSize: 8192\n",
      "25/03/15 07:42:22 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/03/15 07:42:22 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "25/03/15 07:42:22 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://159.89.165.171:4041\n",
      "25/03/15 07:42:22 INFO SparkContext: Added JAR /opt/spark/jars/spark-sql-kafka-0-10_2.12-3.2.1.jar at spark://8b53c5eb6b38:35039/jars/spark-sql-kafka-0-10_2.12-3.2.1.jar with timestamp 1742024542714\n",
      "25/03/15 07:42:22 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...\n",
      "25/03/15 07:42:22 DEBUG TransportClientFactory: Creating new connection to spark-master/172.19.0.8:7077\n",
      "25/03/15 07:42:22 DEBUG TransportClientFactory: Connection to spark-master/172.19.0.8:7077 successful, running bootstraps...\n",
      "25/03/15 07:42:22 INFO TransportClientFactory: Successfully created connection to spark-master/172.19.0.8:7077 after 6 ms (0 ms spent in bootstraps)\n",
      "25/03/15 07:42:22 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250315074222-0002\n",
      "25/03/15 07:42:22 DEBUG TransportServer: Shuffle server started on port: 43525\n",
      "25/03/15 07:42:22 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43525.\n",
      "25/03/15 07:42:22 INFO NettyBlockTransferService: Server created on 8b53c5eb6b38:43525\n",
      "25/03/15 07:42:22 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "25/03/15 07:42:22 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8b53c5eb6b38, 43525, None)\n",
      "25/03/15 07:42:22 DEBUG DefaultTopologyMapper: Got a request for 8b53c5eb6b38\n",
      "25/03/15 07:42:22 INFO BlockManagerMasterEndpoint: Registering block manager 8b53c5eb6b38:43525 with 1048.8 MiB RAM, BlockManagerId(driver, 8b53c5eb6b38, 43525, None)\n",
      "25/03/15 07:42:22 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8b53c5eb6b38, 43525, None)\n",
      "25/03/15 07:42:22 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 8b53c5eb6b38, 43525, None)\n",
      "25/03/15 07:42:22 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "25/03/15 07:42:22 DEBUG SparkContext: Adding shutdown hook\n",
      "25/03/15 07:42:22 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "25/03/15 07:42:22 INFO SharedState: Warehouse path is 'file:/opt/spark/notebooks/spark-warehouse'.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaTest\") \\\n",
    "    .config(\"spark.jars\", \"/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.2.1.jar\") \\\n",
    "    .config(\"spark.executor.extraClassPath\", \"/opt/spark/jars/*\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"/opt/spark/jars/*\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59670faa-3c13-41ed-bb5e-f67da8e68948",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3339603491.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[13], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    ls -lh /opt/spark/jars/spark-sql-kafka-0-10_2.12-3.2.1.jar\u001b[0m\n\u001b[0m                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18ae5c9-26ae-496d-b5c8-06996f293ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
